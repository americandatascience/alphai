{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AlphAI by American Data Science","text":"<p>Welcome to AlphAI.</p>"},{"location":"authentication/","title":"Authentication","text":""},{"location":"authentication/#setup-and-pre-requisites","title":"Setup and Pre-requisites","text":"<p>Although not strictly required to use the computational functions of the alphai package, it is recommended to create an account at American Data Science and generate an API key to make use of your two free remote Jupyter Lab servers.</p> <p>You don't need an API key to use the GPU profiling, benchmarking, and generate modules.</p>"},{"location":"authentication/#get-your-api-key","title":"Get your API key","text":"<p>You can generate your API key after logging in and visiting your profile page here.</p>"},{"location":"authentication/#try-it-out","title":"Try it out","text":"<p>If you're authenticated with your AmDS account and api key, you can start and stop your servers.</p> <pre><code>import os\nfrom alphai import AlphAI\n\naai = AlphAI(\n    api_key=os.environ.get(\"ALPHAI_API_KEY\"),\n)\n\n# Starting default server\n# May take a moment to get ready\naai.start_server()\n\n# Upload to your server's file system \naai.upload(\"./main.py\")\n\n# Start python kernel and run code remotely\ncode = \"print('Hello world!')\"\naai.run_code(code)\n</code></pre> <p>To stop the servers run below:</p> <pre><code>aai.stop_server()\n</code></pre>"},{"location":"benchmarking/","title":"Benchmarking","text":"<p>AlphAI provides a very pythonic and simple approach to time and benchmark your code and callables (functions).</p> <p>Using these tools do not require authentication.</p>"},{"location":"benchmarking/#timing","title":"Timing","text":"<p>Let's instantiate the <code>AlphAI</code> object and create a python function to time and benchmark:</p> <pre><code>import os\nfrom alphai import AlphAI\n\naai = AlphAI()\n</code></pre> <pre><code>import time\n\n# Sleep for x+y seconds\ndef add_sleep(x, y):\n    time.sleep(x+y)\n</code></pre> <p>Now let's see how long it takes to run this function. It should take around 0.055 seconds; simple!</p> <pre><code>x = 0.05\ny = 0.005\n\naai.start_timer()\nadd_sleep(x, y)\naai.stop_timer()\n</code></pre>"},{"location":"benchmarking/#benchmark","title":"Benchmark","text":"<p>Our <code>benchmark()</code> is currently really a timer, but evaluation by comparison is a simple step. Also note that <code>benchmark</code> works with key word arguments as well.</p> <pre><code>x_ = 0.01\ny_ = 0.001\n</code></pre> <pre><code>aai.benchmark(add_sleep, x, y, num_iter=100)\n</code></pre> <pre><code>aai.benchmark(add_sleep, x_, y_, num_iter=100)\n</code></pre>"},{"location":"benchmarking/#key-word-arguments","title":"Key Word Arguments","text":"<pre><code># Sleep for x+y seconds\ndef add_sleep_kw(x, y = 0.005):\n    time.sleep(x+y)\n</code></pre> <pre><code>aai.benchmark(add_sleep_kw, x_, y=y_, num_iter=100)\n</code></pre>"},{"location":"benchmarking/#example-with-pytorch","title":"Example with PyTorch","text":"<p>Why don't we try this on a forward pass of a PyTorch model!</p> <pre><code>aai.start_timer()\noutput = aai.generate(\"Hello!\")\naai.stop_timer()\n</code></pre> <pre><code>aai.benchmark(aai.generate, \"Hello!\", num_iter = 5)\n</code></pre>"},{"location":"gpu-profiling/","title":"Profiling GPU","text":"<p>AlphAI provides a simple and straight forward profiling process to analyze your tensor processes on your GPUs.</p> <p>Using these tools do not require authentication. However, authentication is required for <code>load_view()</code>.</p>"},{"location":"gpu-profiling/#profiling-with-pytorch","title":"Profiling with PyTorch","text":"<p>Let's instantiate the <code>AlphAI</code> object to profile our tensor run with PyTorch.</p> <pre><code>import os\nfrom alphai import AlphAI\n\naai = AlphAI()\n</code></pre>"},{"location":"gpu-profiling/#with-start-and-stop","title":"With start() and stop()","text":"<pre><code>import torch\nimport math\n\naai.start()\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nx = torch.linspace(-math.pi, math.pi, 2000)\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\naai.stop()\n</code></pre>"},{"location":"gpu-profiling/#with-context-manager","title":"With context manager","text":"<pre><code>with aai:\n    model = torch.nn.Sequential(\n        torch.nn.Linear(3, 1),\n        torch.nn.Flatten(0, 1)\n    )\n    x = torch.linspace(-math.pi, math.pi, 2000)\n    p = torch.tensor([1, 2, 3])\n    xx = x.unsqueeze(-1).pow(p)\n</code></pre>"},{"location":"gpu-profiling/#run-profiler-analytics","title":"Run profiler analytics","text":"<pre><code>aai.run_profiler_analysis()\n</code></pre> <pre><code>aai.get_averages()\n</code></pre>"},{"location":"gpu-profiling/#save","title":"Save","text":"<pre><code>aai.save()\n</code></pre>"},{"location":"gpu-profiling/#profile-and-load-view","title":"Profile and Load View","text":"<p>If you'd like to run <code>load_view()</code> and to see your GPU usage statistics and more, you'll need to authenticate.</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Make a `.env` file that contains the following line\n# ALPHAI_API_KEY=&lt;your-api-key&gt;\n\n\nload_dotenv()\n</code></pre> <pre><code>import os\n\nfrom alphai import AlphAI\n\naai = AlphAI(\n    # Don't need this line if you ran load_dotenv()\n    api_key=os.environ.get(\"ALPHAI_API_KEY\"),\n)\n</code></pre> <p>Now just wrap any PyTorch GPU operation you'd like to profile.</p> <pre><code>import math\nimport torch\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n).to(\"cuda\")\nx = torch.linspace(-math.pi, math.pi, 2000)\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p).to(\"cuda\")\n\naai.start()\nmodel(xx)\naai.stop()\n</code></pre> <p>Run profiler analytics and save your trace and analytics locally.</p> <pre><code>aai.run_profiler_analysis()\naai.save()\n</code></pre> <p>Then run load view to use the online viewer for your analytics and statistics.</p> <pre><code>aai.load_view()\n</code></pre>"},{"location":"installation/","title":"Getting Started","text":"<p>AlphAI is a powerful and high-level tool for running analytics, LLMs, and profiling on GPU servers. It also provides you with a client to American Data Science Labs for powerful control over your remote Jupyter Lab servers and environment runtimes.</p> <p>If you're familiar with python, you can install AlphAI with pip, the python package manager.</p>"},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#with-pip","title":"with pip","text":"<p>Install the python package by running <code>pip install alphai</code>.</p> <p>To run profiling and model inference on GPUs with alphai, an installation of PyTorch in a Linux OS with CUDA-enabled is required.</p> <p>Benchmarking, American Data Science client, and other model utilities will work without a GPU, Linux OS, or PyTorch installed.</p> <p>If your CUDA drivers are up to date, you can install alphai with GPU-enabled torch by running <code>pip install alphai[torch]</code>.</p>"},{"location":"installation/#quickstart","title":"quickstart","text":"<p>You can check if AlphAI was successfully installed by trying out the benchmarking tools:</p> <pre><code>from alphai import AlphAI\n\naai = AlphAI()\n\ndef some_function(x, y):\n    return x+y\n\naai.start_timer()\nsome_function(1, 2)\naai.stop_timer()\n\naai.benchmark(some_function, 1, 2, num_iter = 100)\n</code></pre> <p>If you have torch installed, you can even use the <code>generate()</code> feature.</p> <pre><code>prompt = \"Hello there!\"\n\naai.start_timer()\naai.generate(prompt)\naai.stop_timer()\n\naai.benchmark(aai.generate, prompt, num_iter = 5)\n</code></pre>"},{"location":"lab-servers/","title":"Jupyter Lab Servers","text":"<p>AlphAI integrates directly with American Data Science's remote Jupyter Lab servers.</p> <p>You can start servers, stop servers, upload files to your remote file system, load GPU profiling data, and even run code programmatically!</p> <p>Using these tools require authentication.</p>"},{"location":"lab-servers/#american-data-science-client","title":"American Data Science Client","text":"<p>Let's instantiate the <code>AlphAI</code> object to profile our tensor run with PyTorch.</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Make a `.env` file that contains the following line\n# ALPHAI_API_KEY=&lt;your-api-key&gt;\n\n\nload_dotenv()\n</code></pre> <pre><code>import os\nfrom alphai import AlphAI\n\naai = AlphAI(\n    # Don't need this line if you ran load_dotenv()\n    api_key=os.environ.get(\"ALPHAI_API_KEY\"),\n)\n</code></pre>"},{"location":"lab-servers/#start-your-remote-jupyter-lab-servers","title":"Start your remote Jupyter Lab servers","text":"<p>Starting up your servers with AlphAI will use the configurations and environment runtime that were used the last time you started the server from your Dashboard.</p> <p>If it's a new server or you've never started it, it will use the default AI environment runtime.</p> <pre><code>aai.start_server(server_name=\"sandbox\", environment=\"ai\")\n</code></pre> <pre><code>aai.stop_server(server_name=\"sandbox\")\n</code></pre>"},{"location":"lab-servers/#command-agent-alph","title":"Command Agent Alph","text":"<pre><code>response = aai.alph(server_name=\"default\", messages=\"I need a python file that analyzes a generic csv file.\", engine=\"gpt3\")\n</code></pre>"},{"location":"lab-servers/#run-code-remotely","title":"Run code remotely","text":"<p>Alphai allows you to run and \"deploy\" your code given a string or file path. Your server will automatically start a kernel and run your code remotely.</p> <p>All servers will also run a tunnel on port 5000, so you could even check out your running servers and apps hosted directly from the server!</p> <pre><code>hf_code = \"\"\"from transformers import pipeline\nimport gradio as gr\ngr.close_all()\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")\ndemo = gr.Interface.from_pipeline(pipe)\ndemo.launch(server_port=5000, inline=False)\"\"\"\n\naai.run_code(hf_code, clear_other_kernels=True)\n</code></pre> <pre><code>aai.get_service()\n</code></pre>"},{"location":"lab-servers/#upload-local-files-to-server","title":"Upload local files to server","text":"<pre><code>aai.upload(file_path=\"./test.py\")\n</code></pre>"},{"location":"api-reference/alphai/","title":"AlphAI","text":"<p>The AlphAI class provides a high-level interface for benchmarking, memory estimation, and interaction with remote Jupyter Lab servers. It supports various tensor-based models and integrates with American Data Science Labs for managing GPU resources.</p> <p>Attributes:</p> Name Type Description <code>output_path</code> <code>str</code> <p>The path where output files are stored.</p> <code>supported_backends</code> <code>List[str]</code> <p>List of supported tensor backends (e.g., 'torch', 'jax').</p> <code>profiler_started</code> <code>bool</code> <p>Flag to indicate if the profiler has started.</p> <code>server_name</code> <code>str</code> <p>The name of the server for remote operations.</p> <code>api_key</code> <code>str</code> <p>API key for authentication with remote services.</p> <code>client</code> <code>Client</code> <p>Client instance for interacting with remote services.</p> <code>pt_profiler</code> <code>PyTorchProfiler</code> <p>Profiler instance for PyTorch.</p> <code>jax_profiler</code> <code>JaxProfiler</code> <p>Profiler instance for JAX.</p> <code>benchmarker</code> <code>Benchmarker</code> <p>Benchmarker instance for performance measurements.</p> <code>model</code> <code>torch.nn.Module</code> <p>The loaded PyTorch model.</p> <code>model_name_or_path</code> <code>str</code> <p>The name or path of the model.</p> Source code in <code>alphai/alphai.py</code> <pre><code>class AlphAI:\n    \"\"\"\n    The AlphAI class provides a high-level interface for benchmarking, memory estimation,\n    and interaction with remote Jupyter Lab servers. It supports various tensor-based models\n    and integrates with American Data Science Labs for managing GPU resources.\n\n    Attributes:\n        output_path (str): The path where output files are stored.\n        supported_backends (List[str]): List of supported tensor backends (e.g., 'torch', 'jax').\n        profiler_started (bool): Flag to indicate if the profiler has started.\n        server_name (str): The name of the server for remote operations.\n        api_key (str): API key for authentication with remote services.\n        client (Client): Client instance for interacting with remote services.\n        pt_profiler (PyTorchProfiler): Profiler instance for PyTorch.\n        jax_profiler (JaxProfiler): Profiler instance for JAX.\n        benchmarker (Benchmarker): Benchmarker instance for performance measurements.\n        model (torch.nn.Module): The loaded PyTorch model.\n        model_name_or_path (str): The name or path of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        api_key: Union[str, None] = None,\n        organization: Union[str, None] = None,\n        base_url: str = None,\n        output_path: str = \"./alphai_profiler_store\",\n        server_name: str = \"\",\n        pt_profiler_configs: BaseProfilerConfigs = None,\n        jax_profiler_configs: BaseProfilerConfigs = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the AlphAI instance with provided configurations.\n\n        Args:\n            api_key (Union[str, None]): API key for authentication. If None, will try to read from environment.\n            organization (Union[str, None]): The name of the organization. If None, will try to read from environment.\n            base_url (str): The base URL for remote services. If None, defaults to a predefined URL.\n            output_path (str): The path where output files are stored. Defaults to './alphai_profiler_store'.\n            server_name (str): The name of the server for remote operations.\n            pt_profiler_configs (BaseProfilerConfigs): Configuration for the PyTorch profiler.\n            jax_profiler_configs (BaseProfilerConfigs): Configuration for the JAX profiler.\n        \"\"\"\n\n        self.output_path = output_path\n        self.supported_backends = [\"torch\", \"jax\", \"tensorflow\"]\n        self.profiler_started = False\n        self.server_name = server_name\n\n        # Api\n        if api_key is None:\n            api_key = os.environ.get(\"ALPHAI_API_KEY\")\n        if api_key is None:\n            logging.info(\n                \"Optional: Set the API key api_key parameter init or by setting the ALPHAI_API_KEY environment variable\"\n            )\n        self.api_key = api_key\n        if api_key:\n            self.client = Client(access_token=api_key)\n\n        if organization is None:\n            organization = os.environ.get(\"ALPHAI_ORGANIZATION_NAME\")\n        self.organization = organization\n\n        if base_url is None:\n            base_url = os.environ.get(\"ALPHAI_BASE_URL\")\n        if base_url is None:\n            base_url = f\"https://lab.amdatascience.com\"\n        self.base_url = base_url\n\n        # Directory ops\n        self.pt_trace_dirs = self.get_pt_traces()\n\n        # Profilers\n        self.dict_idle_time = None\n        self.dict_averages = None\n\n        if is_package_installed(\"torch\") and not pt_profiler_configs:\n            from alphai.profilers.pytorch import PyTorchProfilerConfigs, PyTorchProfiler\n\n            pt_profiler_configs = PyTorchProfilerConfigs()\n            pt_profiler_configs.dir_path = output_path\n            self.pt_profiler = PyTorchProfiler(pt_profiler_configs)\n\n        if is_package_installed(\"jax\") and not jax_profiler_configs:\n            from alphai.profilers.jax import JaxProfilerConfigs, JaxProfiler\n\n            jax_profiler_configs = JaxProfilerConfigs()\n            jax_profiler_configs.dir_path = output_path\n            self.jax_profiler = JaxProfiler(jax_profiler_configs)\n\n        # Benchmarker\n        self.benchmarker = Benchmarker()\n\n        # HF Generate\n        self.model_name_or_path = None\n        self.model = None\n\n    def start(self, tensor_backend: str = None):\n        \"\"\"\n        Starts the profiler for the specified tensor backend.\n\n        Args:\n            tensor_backend (str): The backend to use for profiling ('torch', 'jax', 'tensorflow').\n                                   If None, defaults to an available backend.\n        \"\"\"\n        # Handle if none, not installed, or unknown tensor_backend given\n        # Default to torch tensorbackend or whatever's available\n        if not tensor_backend:\n            if is_package_installed(\"torch\"):\n                tensor_backend = \"torch\"\n            elif is_package_installed(\"jax\"):\n                tensor_backend = \"jax\"\n            elif is_package_installed(\"tensorflow\"):\n                tensor_backend = \"tensorflow\"\n            else:\n                warnings.warn(\n                    f\"Tensor framework must first be installed from a supported library: {self.supported_backends} to enable profiling.\"\n                )\n                return\n        if tensor_backend not in self.supported_backends:\n            warnings.warn(\n                f\"Tensor framework is not supported, must be one of {self.supported_backends} to enable profiling.\"\n            )\n            return\n        if not is_package_installed(tensor_backend):\n            warnings.warn(f\"You need to install '{tensor_backend}' to start profiling\")\n\n        if tensor_backend == \"torch\":\n            try:\n                self.pt_profiler.start()\n            except:\n                # Try to stop hanging profiler and try again\n                self.pt_profiler.stop()\n                self.pt_profiler.start()\n        elif tensor_backend == \"jax\":\n            try:\n                self.jax_profiler.start()\n            except:\n                # Try to stop hanging profiler and try again\n                self.jax_profiler.stop()\n                self.jax_profiler.start()\n        elif tensor_backend == \"tensorflow\":\n            pass\n\n        self.tensor_backend = tensor_backend\n        self.profiler_started = True\n\n    def stop(self):\n        \"\"\"\n        Stops the currently running profiler.\n        \"\"\"\n        if not self.profiler_started or not self.tensor_backend:\n            warnings.warn(f\"Profiler never started\")\n            return\n\n        if self.tensor_backend == \"torch\":\n            self.pt_profiler.stop()\n        elif self.tensor_backend == \"jax\":\n            self.jax_profiler.stop()\n        elif self.tensor_backend == \"tensorflow\":\n            pass\n\n        self.profiler_started = False\n\n    def step(self):\n        \"\"\"\n        Advances the profiler by one step. Mainly used for the PyTorch profiler.\n        \"\"\"\n        self.pt_profiler.step()\n\n    def __call__(self, tensor_backend: str = None):\n        # Allows for param in context manager\n        # self.tensor_backend only set with context manager or in start()\n        self.tensor_backend = tensor_backend\n        return self\n\n    def __enter__(self):\n        self.start(tensor_backend=self.tensor_backend)\n\n    def __exit__(self, exc_type, exc_val, exc_t):\n        self.stop()\n\n    # API Methods\n    def get_servers(self):\n        \"\"\"\n        Retrieves the list of available servers from the remote service.\n\n        Returns:\n            A list of servers if successful, or raises an exception if the user is not authenticated.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        return self.client.get_servers()\n\n    def start_server(\n            self,\n            server_name: str = None,\n            environment: str = \"ai\",\n            server_request: str = \"medium-cpu\",\n        ):\n        \"\"\"\n        Starts a server with the given name.\n\n        Args:\n            server_name (str): The name of the server to start. If None, uses the server name set in the instance.\n\n        Returns:\n            Response from the server start request.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        # Use set self.server_name if not provided\n        if server_name is None:\n            server_name = self.server_name\n        return self.client.start_server(server_name=server_name, environment=environment, server_request=server_request)\n\n    def stop_server(self, server_name: str = None):\n        \"\"\"\n        Stops a server with the given name.\n\n        Args:\n            server_name (str): The name of the server to stop. If None, uses the server name set in the instance.\n\n        Returns:\n            Response from the server stop request.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        # Use set self.server_name if not provided\n        if server_name is None:\n            server_name = self.server_name\n        return self.client.stop_server(server_name=server_name)\n\n    def alph(\n            self,\n            server_name: str = None,\n            messages: str = \"ls\",\n            engine: str = \"gpt3\",\n        ):\n        \"\"\"\n        Gives alph commands to help you and run on the server.\n\n        Args:\n            server_name (str): The name of the server to stop. If None, uses the server name set in the instance.\n\n        Returns:\n            Response from the server stop request.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        # Use set self.server_name if not provided\n        if server_name is None:\n            server_name = self.server_name\n        return self.client.alph(server_name=server_name, messages=messages, engine=engine)\n\n    def upload(self, server_name: str = None, file_path: str = \"\", remote_path=\"\"):\n        \"\"\"\n        Uploads a file to a remote server.\n\n        Args:\n            server_name (str): The name of the server to which the file will be uploaded. If None, uses the server name set in the instance.\n            file_path (str): The local path to the file.\n            remote_path (str): The remote path where the file will be stored.\n\n        Returns:\n            The response from the upload request.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        # Use set self.server_name if not provided\n        if server_name is None:\n            server_name = self.server_name\n        return self.client.put_contents(\n            server_name=server_name, path=remote_path, file_path=file_path\n        )\n\n    def run_code(\n        self,\n        code: str = \"print('Hello world!')\",\n        server_name: str = None,\n        clear_other_kernels: bool = True,\n        return_full: bool = False,\n    ):\n        \"\"\"\n        Executes the given code on a remote server.\n\n        Args:\n            code (str): The code to execute. If a file path is provided, the code in the file is executed.\n            server_name (str): The name of the server where the code will be executed. If None, uses the server name set in the instance.\n            clear_other_kernels (bool): Whether to shut down other kernels on the server before executing the code.\n            return_full (bool): Whether to return the full response from the server.\n\n        Returns:\n            The output from the code execution.\n        \"\"\"\n        # Use set self.server_name if not provided\n        if server_name is None:\n            server_name = self.server_name\n        if clear_other_kernels:\n            self.client.shutdown_all_kernels(server_name=server_name)\n        if os.path.isfile(code):\n            if os.path.splitext(code)[1] != \".py\":\n                warnings.warn(\n                    \"This doesn't seem to be a python file, but will try to run it anyway.\"\n                )\n            with open(code, \"r\") as f:\n                code = f.read()\n        return self.client.send_channel_execute(\n            server_name=server_name, messages=[code], return_full=return_full\n        )\n\n    def get_service(self, server_name: str = None):\n        \"\"\"\n        Retrieves the service URL for a running service or app on the server.\n\n        Args:\n            server_name (str): The name of the server. If None, uses the server name set in the instance.\n\n        Returns:\n            The URL to access the running service or app on the server.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        if server_name is None:\n            server_name = self.server_name\n        return f\"If you have running service or app in your server, check it out here -&gt; {self.client.get_service(server_name=server_name)}\"\n\n    # Profilers\n    def get_profiler_stats(self):\n        \"\"\"\n        Retrieves statistics from the PyTorch profiler.\n\n        Returns:\n            A table containing key averages of profiler statistics, particularly focusing on CUDA time.\n        \"\"\"\n        stat_table = self.pt_profiler.key_averages().table(\n            sort_by=\"cuda_time_total\", row_limit=10\n        )\n        return stat_table\n\n    def get_averages(\n        self,\n        sort_by=\"cuda_time_total\",\n        header=None,\n        row_limit=100,\n        max_src_column_width=75,\n        max_name_column_width=55,\n        max_shapes_column_width=80,\n        top_level_events_only=False,\n    ):\n        \"\"\"\n        Retrieves a DataFrame of average statistics from the PyTorch profiler powered by Kineto.\n\n        Args:\n            sort_by (str): The attribute to sort the data by. Defaults to 'cuda_time_total'.\n            header (str, optional): Header for the DataFrame. Defaults to None.\n            row_limit (int): The maximum number of rows to return. Defaults to 100.\n            max_src_column_width (int): Maximum width for the source column. Defaults to 75.\n            max_name_column_width (int): Maximum width for the name column. Defaults to 55.\n            max_shapes_column_width (int): Maximum width for the shapes column. Defaults to 80.\n            top_level_events_only (bool): Whether to include only top-level events. Defaults to False.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the averaged profiler statistics.\n        \"\"\"\n        df_averages, self.dict_averages, str_averages = self.pt_profiler.get_averages(\n            sort_by=\"cuda_time_total\",\n            header=None,\n            row_limit=100,\n            max_src_column_width=75,\n            max_name_column_width=55,\n            max_shapes_column_width=80,\n            top_level_events_only=False,\n        )\n        return df_averages\n\n    def run_profiler_analysis(self, trace_path: str = None, visualize: bool = False):\n        \"\"\"\n        Runs an analysis of the profiler data and optionally visualizes the results.\n\n        Args:\n            trace_path (str, optional): The path to the trace data. If None, uses the latest trace. Defaults to None.\n            visualize (bool): Whether to visualize the analysis results. Defaults to False.\n\n        Returns:\n            A tuple of DataFrames containing various analysis results, such as idle time, temporal breakdown, and GPU kernel breakdown.\n        \"\"\"\n        if trace_path:\n            pt_trace_dirs = [trace_path]\n        else:\n            pt_trace_dirs = self.get_pt_traces()\n        if pt_trace_dirs:\n            try:\n                trace_dir = os.path.join(self.pt_profiler.dir_path, pt_trace_dirs[-1])\n                self.analyzer = TraceAnalysis(trace_dir=trace_dir)\n                idle_time_df = self.analyzer.get_idle_time_breakdown(\n                    show_idle_interval_stats=True, visualize=visualize\n                )\n                time_spent_df = self.analyzer.get_temporal_breakdown(\n                    visualize=visualize\n                )\n                (\n                    kernel_type_metrics_df,\n                    kernel_metrics_df,\n                ) = self.analyzer.get_gpu_kernel_breakdown()\n                self.dict_idle_time = idle_time_df[0].to_dict()\n                self.dict_time_spent = time_spent_df.to_dict()\n                self.dict_type_metrics = kernel_type_metrics_df.to_dict()\n                self.dict_kernel_metrics = kernel_metrics_df.to_dict()\n                return (\n                    idle_time_df,\n                    time_spent_df,\n                    kernel_type_metrics_df,\n                    kernel_metrics_df,\n                )\n            except:\n                warnings.warn(\n                    \"Error running profiler analysis, may not have GPU trace data so will continue without it.\"\n                )\n                self.dict_idle_time = {}\n                self.dict_time_spent = {}\n                self.dict_type_metrics = {}\n                self.dict_kernel_metrics = {}\n                return\n\n    def save(self, return_results: bool = False):\n        \"\"\"\n        Saves the profiler data and analysis results to a specified directory.\n\n        Args:\n            return_results (bool): Whether to return the saved data as a dictionary. Defaults to False.\n\n        Returns:\n            dict (optional): A dictionary containing the saved data if return_results is True.\n        \"\"\"\n        alphai_dict = {}\n        if self.dict_idle_time is None:\n            warnings.warn(\n                \"Make sure to run_profiler_analysis() before saving to your analytics.\"\n            )\n            self.run_profiler_analysis()\n        self.get_averages()\n        alphai_dict[\"metadata\"] = self.analyzer.t.meta_data\n        alphai_dict[\"idle_time\"] = self.dict_idle_time\n        alphai_dict[\"time_spent\"] = self.dict_time_spent\n        alphai_dict[\"type_metrics\"] = self.dict_type_metrics\n        alphai_dict[\"kernel_metrics\"] = self.dict_kernel_metrics\n        alphai_dict[\"key_averages\"] = self.dict_averages\n        with open(\n            os.path.join(self.pt_profiler.profiler_path, \"profiling.alphai\"), \"w\"\n        ) as f:\n            json.dump(alphai_dict, f, indent=4)\n        if return_results:\n            return alphai_dict\n\n    def load_view(self, dir_name: str = None):\n        \"\"\"\n        Loads a view of the profiler data onto your remote server.\n\n        Args:\n            dir_name (str, optional): The directory name to load the view from. If None, generates a timestamp-based directory name. Defaults to None.\n\n        Returns:\n            str: A URL to the GPU usage statistics dashboard.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"Requires user authentication with an API Key\")\n        formatted_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        if not dir_name:\n            view_path = f\"{formatted_datetime}.alphai\"\n        else:\n            view_path = dir_name\n        self.client.post_contents(path=\"\", ext=\".alphai\", type=\"directory\")\n        self.client.patch_contents(path=\"Untitled Folder.alphai\", new_path=view_path)\n        self.client.put_contents(\n            path=view_path,\n            file_path=f\"{self.pt_profiler.profiler_path}/profiling.alphai\",\n        )\n        return f\"Check out your GPU usage statistics at -&gt; https://dashboard.amdatascience.com/agent-alph\"\n\n    def get_pt_traces(self):\n        \"\"\"\n        Retrieves a list of PyTorch trace directories sorted by date.\n\n        Returns:\n            List[str]: A list of directory names containing PyTorch traces.\n        \"\"\"\n        # List all items in the directory\n        directory_path = self.output_path\n        if not os.path.isdir(directory_path):\n            return []\n        all_items = os.listdir(directory_path)\n\n        # Filter out items that are directories and follow the naming pattern\n        date_directories = []\n        for item in all_items:\n            if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(\n                \"pt_trace_\"\n            ):\n                # Extract the date and time part from the folder name\n                datetime_part = item.split(\"pt_trace_\")[1]\n\n                # Parse the date and time into a datetime object\n                try:\n                    folder_date = datetime.datetime.strptime(\n                        datetime_part, \"%Y-%m-%d_%H-%M-%S\"\n                    )\n                    date_directories.append((item, folder_date))\n                except ValueError:\n                    # Handle cases where the date format is incorrect or different\n                    print(f\"Skipping {item} due to unexpected date format.\")\n\n        # Sort the directories by the parsed datetime\n        date_directories.sort(key=lambda x: x[1])\n\n        # Return only the directory names, in sorted order\n        return [name for name, date in date_directories]\n\n    def get_jax_traces(self):\n        \"\"\"\n        Retrieves a list of JAX trace directories sorted by date.\n\n        Returns:\n            List[str]: A list of directory names containing JAX traces.\n        \"\"\"\n        # List all items in the directory\n        directory_path = self.output_path\n        if not os.path.isdir(directory_path):\n            return []\n        all_items = os.listdir(directory_path)\n\n        # Filter out items that are directories and follow the naming pattern\n        date_directories = []\n        for item in all_items:\n            if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(\n                \"jax_trace_\"\n            ):\n                # Extract the date and time part from the folder name\n                datetime_part = item.split(\"jax_trace_\")[1]\n\n                # Parse the date and time into a datetime object\n                try:\n                    folder_date = datetime.datetime.strptime(\n                        datetime_part, \"%Y-%m-%d_%H-%M-%S\"\n                    )\n                    date_directories.append((item, folder_date))\n                except ValueError:\n                    # Handle cases where the date format is incorrect or different\n                    print(f\"Skipping {item} due to unexpected date format.\")\n\n        # Sort the directories by the parsed datetime\n        date_directories.sort(key=lambda x: x[1])\n\n        # Return only the directory names, in sorted order\n        return [name for name, date in date_directories]\n\n    # Benchmarker\n    def start_timer(self):\n        \"\"\"\n        Starts the benchmarking timer.\n        \"\"\"\n        self.benchmarker.start()\n\n    def stop_timer(self, print_results: bool = True):\n        \"\"\"\n        Stops the timer and optionally prints the results.\n\n        Args:\n            print_results (bool): Whether to print the results. Defaults to True.\n\n        Returns:\n            The results of the benchmark.\n        \"\"\"\n        return self.benchmarker.stop()\n\n    def benchmark(\n        self,\n        function: Callable = None,\n        *args,\n        num_iter: int = 100,\n        print_results: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Benchmarks a function by running it a specified number of times.\n\n        Args:\n            function (Callable): The function to benchmark.\n            *args: The arguments to pass to the function.\n            num_iter (int): The number of times to run the function. Defaults to 100.\n            print_results (bool): Whether to print the results. Defaults to True.\n            **kwargs: The keyword arguments to pass to the function.\n\n        Returns:\n            The results of the benchmark.\n        \"\"\"\n        return self.benchmarker.benchmark(\n            function, *args, num_iter=num_iter, print_results=print_results, **kwargs\n        )\n\n    # Hugging Face utility\n\n    def estimate_memory_requirement(\n        self,\n        model_name: str = \"stabilityai/stablelm-zephyr-3b\",\n    ):\n        \"\"\"\n        Estimates the memory requirement for a given model.\n\n        Args:\n            model_name (str): The name of the model. Defaults to \"stabilityai/stablelm-zephyr-3b\".\n\n        Returns:\n            A dictionary with the model name and the estimated memory requirement in MB and GB.\n        \"\"\"\n        try:\n            param_value = extract_param_value(model_name)\n            megabyte_value = param_value * 2 * 1000\n            gigabyte_value = param_value * 2\n            print(\n                f\"Estimated memory requirement assuming float16 dtype for {model_name}: {megabyte_value:.2f} MB or {gigabyte_value:.2f} GB\"\n            )\n            return {\n                \"model_name_or_path\": model_name,\n                \"estimate_memory_requirement_mb_float16\": f\"{megabyte_value:.2f} MB\",\n                \"estimate_memory_requirement_gb_float16\": f\"{gigabyte_value:.2f} GB\",\n            }\n        except:\n            warnings.warn(\n                \"Error parsing model name or path, can't estimate memory requirement.\"\n            )\n            return\n\n    def memory_requirement(\n        self,\n        model_name_or_path: str = \"stabilityai/stablelm-zephyr-3b\",\n        device: str = \"cuda\",\n        trust_remote_code=True,\n        torch_dtype=\"auto\",\n    ):\n        \"\"\"\n        Estimates and prints the memory requirement for a specified model.\n\n        Args:\n            model_name_or_path (str): The name or path of the model to be loaded. Defaults to 'stabilityai/stablelm-zephyr-3b'.\n            device (str): The device to load the model on ('cuda' or 'cpu'). Defaults to 'cuda'.\n            trust_remote_code (bool): Whether to trust remote code when loading the model. Defaults to True.\n            torch_dtype (str): The data type for the model parameters. Defaults to 'auto'.\n\n        Returns:\n            dict: A dictionary containing the memory requirement in MB and GB.\n        \"\"\"\n        if not is_package_installed(\"torch\"):\n            warnings.warn(f\"You need to install 'torch' to run memory_requirement\")\n            return\n        if not self.model_name_or_path or self.model_name_or_path != model_name_or_path:\n            self.model_name_or_path = model_name_or_path\n            try:\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=trust_remote_code,\n                    torch_dtype=torch_dtype,\n                ).to(device)\n            except:\n                warnings.warn(\n                    \"Loading model to CPU instead of GPU since GPU is not available.\"\n                )\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=trust_remote_code,\n                    torch_dtype=torch_dtype,\n                ).to(\"cpu\")\n        try:\n            param_value = self.model.num_parameters()\n        except:\n            param_value = sum(p.numel() for p in self.model.parameters())\n\n        megabyte_value = param_value * 2 / 1000000\n        gigabyte_value = param_value * 2 / 1000000000\n        print(\n            f\"Memory requirement assuming float16 dtype for {model_name_or_path}: {megabyte_value:.2f} MB or {gigabyte_value:.2f} GB\"\n        )\n        return {\n            \"model_name_or_path\": model_name_or_path,\n            \"memory_requirement_mb_float16\": f\"{megabyte_value:.2f} MB\",\n            \"memory_requirement_gb_float16\": f\"{gigabyte_value:.2f} GB\",\n        }\n\n    def generate(\n        self,\n        text: str = \"\",\n        prompt: List[dict] = None,\n        model_name_or_path: str = \"stabilityai/stablelm-zephyr-3b\",\n        trust_remote_code=True,\n        torch_dtype=\"auto\",\n        stream: bool = True,\n        device: str = \"cuda\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generates text using the specified model based on the given prompt or text.\n\n        Args:\n            text (str): The text to be used as a prompt. Defaults to an empty string.\n            prompt (List[dict]): A list of dictionaries defining the prompt structure. Defaults to None.\n            model_name_or_path (str): The name or path of the model to be used. Defaults to 'stabilityai/stablelm-zephyr-3b'.\n            trust_remote_code (bool): Whether to trust remote code when loading the model. Defaults to True.\n            torch_dtype (str): The data type for the model parameters. Defaults to 'auto'.\n            stream (bool): Whether to use streaming for text generation. Defaults to True.\n            device (str): The device to run the model on. Defaults to 'cuda'.\n\n        Returns:\n            str: The generated text.\n        \"\"\"\n        if not is_package_installed(\"torch\"):\n            warnings.warn(f\"You need to install 'torch' to run generate\")\n            return\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        streamer = TextStreamer(tokenizer) if stream else None\n        if not self.model_name_or_path or self.model_name_or_path != model_name_or_path:\n            self.model_name_or_path = model_name_or_path\n            try:\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=trust_remote_code,\n                    torch_dtype=torch_dtype,\n                ).to(device)\n            except:\n                warnings.warn(\n                    \"Loading model to CPU instead of GPU since GPU is not available.\"\n                )\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    model_name_or_path,\n                    trust_remote_code=trust_remote_code,\n                    torch_dtype=torch_dtype,\n                ).to(\"cpu\")\n\n        if not prompt:\n            prompt = [{\"role\": \"user\", \"content\": text}]\n        inputs = tokenizer.apply_chat_template(\n            prompt, add_generation_prompt=True, return_tensors=\"pt\"\n        )\n\n        tokens = self.model.generate(\n            inputs.to(self.model.device),\n            max_new_tokens=1024,\n            temperature=0.8,\n            do_sample=True,\n            streamer=streamer,\n            **kwargs,\n        )\n\n        return tokenizer.decode(tokens[0])\n\n    def clear_cuda_memory(self):\n        \"\"\"\n        Clears the CUDA memory cache to free up GPU memory.\n\n        Raises:\n            Warning: If PyTorch is not installed.\n        \"\"\"\n        if not is_package_installed(\"torch\"):\n            warnings.warn(f\"You need to install 'torch' to run clear_cuda_memory\")\n            return\n        gc.collect()\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.__init__","title":"<code>__init__(self, *, api_key=None, organization=None, base_url=None, output_path='./alphai_profiler_store', server_name='', pt_profiler_configs=None, jax_profiler_configs=None, **kwargs)</code>  <code>special</code>","text":"<p>Initializes the AlphAI instance with provided configurations.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Union[str, None]</code> <p>API key for authentication. If None, will try to read from environment.</p> <code>None</code> <code>organization</code> <code>Union[str, None]</code> <p>The name of the organization. If None, will try to read from environment.</p> <code>None</code> <code>base_url</code> <code>str</code> <p>The base URL for remote services. If None, defaults to a predefined URL.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>The path where output files are stored. Defaults to './alphai_profiler_store'.</p> <code>'./alphai_profiler_store'</code> <code>server_name</code> <code>str</code> <p>The name of the server for remote operations.</p> <code>''</code> <code>pt_profiler_configs</code> <code>BaseProfilerConfigs</code> <p>Configuration for the PyTorch profiler.</p> <code>None</code> <code>jax_profiler_configs</code> <code>BaseProfilerConfigs</code> <p>Configuration for the JAX profiler.</p> <code>None</code> Source code in <code>alphai/alphai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_key: Union[str, None] = None,\n    organization: Union[str, None] = None,\n    base_url: str = None,\n    output_path: str = \"./alphai_profiler_store\",\n    server_name: str = \"\",\n    pt_profiler_configs: BaseProfilerConfigs = None,\n    jax_profiler_configs: BaseProfilerConfigs = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the AlphAI instance with provided configurations.\n\n    Args:\n        api_key (Union[str, None]): API key for authentication. If None, will try to read from environment.\n        organization (Union[str, None]): The name of the organization. If None, will try to read from environment.\n        base_url (str): The base URL for remote services. If None, defaults to a predefined URL.\n        output_path (str): The path where output files are stored. Defaults to './alphai_profiler_store'.\n        server_name (str): The name of the server for remote operations.\n        pt_profiler_configs (BaseProfilerConfigs): Configuration for the PyTorch profiler.\n        jax_profiler_configs (BaseProfilerConfigs): Configuration for the JAX profiler.\n    \"\"\"\n\n    self.output_path = output_path\n    self.supported_backends = [\"torch\", \"jax\", \"tensorflow\"]\n    self.profiler_started = False\n    self.server_name = server_name\n\n    # Api\n    if api_key is None:\n        api_key = os.environ.get(\"ALPHAI_API_KEY\")\n    if api_key is None:\n        logging.info(\n            \"Optional: Set the API key api_key parameter init or by setting the ALPHAI_API_KEY environment variable\"\n        )\n    self.api_key = api_key\n    if api_key:\n        self.client = Client(access_token=api_key)\n\n    if organization is None:\n        organization = os.environ.get(\"ALPHAI_ORGANIZATION_NAME\")\n    self.organization = organization\n\n    if base_url is None:\n        base_url = os.environ.get(\"ALPHAI_BASE_URL\")\n    if base_url is None:\n        base_url = f\"https://lab.amdatascience.com\"\n    self.base_url = base_url\n\n    # Directory ops\n    self.pt_trace_dirs = self.get_pt_traces()\n\n    # Profilers\n    self.dict_idle_time = None\n    self.dict_averages = None\n\n    if is_package_installed(\"torch\") and not pt_profiler_configs:\n        from alphai.profilers.pytorch import PyTorchProfilerConfigs, PyTorchProfiler\n\n        pt_profiler_configs = PyTorchProfilerConfigs()\n        pt_profiler_configs.dir_path = output_path\n        self.pt_profiler = PyTorchProfiler(pt_profiler_configs)\n\n    if is_package_installed(\"jax\") and not jax_profiler_configs:\n        from alphai.profilers.jax import JaxProfilerConfigs, JaxProfiler\n\n        jax_profiler_configs = JaxProfilerConfigs()\n        jax_profiler_configs.dir_path = output_path\n        self.jax_profiler = JaxProfiler(jax_profiler_configs)\n\n    # Benchmarker\n    self.benchmarker = Benchmarker()\n\n    # HF Generate\n    self.model_name_or_path = None\n    self.model = None\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.alph","title":"<code>alph(self, server_name=None, messages='ls', engine='gpt3')</code>","text":"<p>Gives alph commands to help you and run on the server.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The name of the server to stop. If None, uses the server name set in the instance.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the server stop request.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def alph(\n        self,\n        server_name: str = None,\n        messages: str = \"ls\",\n        engine: str = \"gpt3\",\n    ):\n    \"\"\"\n    Gives alph commands to help you and run on the server.\n\n    Args:\n        server_name (str): The name of the server to stop. If None, uses the server name set in the instance.\n\n    Returns:\n        Response from the server stop request.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    # Use set self.server_name if not provided\n    if server_name is None:\n        server_name = self.server_name\n    return self.client.alph(server_name=server_name, messages=messages, engine=engine)\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.benchmark","title":"<code>benchmark(self, function=None, *args, *, num_iter=100, print_results=True, **kwargs)</code>","text":"<p>Benchmarks a function by running it a specified number of times.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to benchmark.</p> <code>None</code> <code>*args</code> <p>The arguments to pass to the function.</p> <code>()</code> <code>num_iter</code> <code>int</code> <p>The number of times to run the function. Defaults to 100.</p> <code>100</code> <code>print_results</code> <code>bool</code> <p>Whether to print the results. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The results of the benchmark.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def benchmark(\n    self,\n    function: Callable = None,\n    *args,\n    num_iter: int = 100,\n    print_results: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Benchmarks a function by running it a specified number of times.\n\n    Args:\n        function (Callable): The function to benchmark.\n        *args: The arguments to pass to the function.\n        num_iter (int): The number of times to run the function. Defaults to 100.\n        print_results (bool): Whether to print the results. Defaults to True.\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        The results of the benchmark.\n    \"\"\"\n    return self.benchmarker.benchmark(\n        function, *args, num_iter=num_iter, print_results=print_results, **kwargs\n    )\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.clear_cuda_memory","title":"<code>clear_cuda_memory(self)</code>","text":"<p>Clears the CUDA memory cache to free up GPU memory.</p> <p>Exceptions:</p> Type Description <code>Warning</code> <p>If PyTorch is not installed.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def clear_cuda_memory(self):\n    \"\"\"\n    Clears the CUDA memory cache to free up GPU memory.\n\n    Raises:\n        Warning: If PyTorch is not installed.\n    \"\"\"\n    if not is_package_installed(\"torch\"):\n        warnings.warn(f\"You need to install 'torch' to run clear_cuda_memory\")\n        return\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.estimate_memory_requirement","title":"<code>estimate_memory_requirement(self, model_name='stabilityai/stablelm-zephyr-3b')</code>","text":"<p>Estimates the memory requirement for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model. Defaults to \"stabilityai/stablelm-zephyr-3b\".</p> <code>'stabilityai/stablelm-zephyr-3b'</code> <p>Returns:</p> Type Description <p>A dictionary with the model name and the estimated memory requirement in MB and GB.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def estimate_memory_requirement(\n    self,\n    model_name: str = \"stabilityai/stablelm-zephyr-3b\",\n):\n    \"\"\"\n    Estimates the memory requirement for a given model.\n\n    Args:\n        model_name (str): The name of the model. Defaults to \"stabilityai/stablelm-zephyr-3b\".\n\n    Returns:\n        A dictionary with the model name and the estimated memory requirement in MB and GB.\n    \"\"\"\n    try:\n        param_value = extract_param_value(model_name)\n        megabyte_value = param_value * 2 * 1000\n        gigabyte_value = param_value * 2\n        print(\n            f\"Estimated memory requirement assuming float16 dtype for {model_name}: {megabyte_value:.2f} MB or {gigabyte_value:.2f} GB\"\n        )\n        return {\n            \"model_name_or_path\": model_name,\n            \"estimate_memory_requirement_mb_float16\": f\"{megabyte_value:.2f} MB\",\n            \"estimate_memory_requirement_gb_float16\": f\"{gigabyte_value:.2f} GB\",\n        }\n    except:\n        warnings.warn(\n            \"Error parsing model name or path, can't estimate memory requirement.\"\n        )\n        return\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.generate","title":"<code>generate(self, text='', prompt=None, model_name_or_path='stabilityai/stablelm-zephyr-3b', trust_remote_code=True, torch_dtype='auto', stream=True, device='cuda', **kwargs)</code>","text":"<p>Generates text using the specified model based on the given prompt or text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be used as a prompt. Defaults to an empty string.</p> <code>''</code> <code>prompt</code> <code>List[dict]</code> <p>A list of dictionaries defining the prompt structure. Defaults to None.</p> <code>None</code> <code>model_name_or_path</code> <code>str</code> <p>The name or path of the model to be used. Defaults to 'stabilityai/stablelm-zephyr-3b'.</p> <code>'stabilityai/stablelm-zephyr-3b'</code> <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading the model. Defaults to True.</p> <code>True</code> <code>torch_dtype</code> <code>str</code> <p>The data type for the model parameters. Defaults to 'auto'.</p> <code>'auto'</code> <code>stream</code> <code>bool</code> <p>Whether to use streaming for text generation. Defaults to True.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to run the model on. Defaults to 'cuda'.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def generate(\n    self,\n    text: str = \"\",\n    prompt: List[dict] = None,\n    model_name_or_path: str = \"stabilityai/stablelm-zephyr-3b\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    stream: bool = True,\n    device: str = \"cuda\",\n    **kwargs,\n):\n    \"\"\"\n    Generates text using the specified model based on the given prompt or text.\n\n    Args:\n        text (str): The text to be used as a prompt. Defaults to an empty string.\n        prompt (List[dict]): A list of dictionaries defining the prompt structure. Defaults to None.\n        model_name_or_path (str): The name or path of the model to be used. Defaults to 'stabilityai/stablelm-zephyr-3b'.\n        trust_remote_code (bool): Whether to trust remote code when loading the model. Defaults to True.\n        torch_dtype (str): The data type for the model parameters. Defaults to 'auto'.\n        stream (bool): Whether to use streaming for text generation. Defaults to True.\n        device (str): The device to run the model on. Defaults to 'cuda'.\n\n    Returns:\n        str: The generated text.\n    \"\"\"\n    if not is_package_installed(\"torch\"):\n        warnings.warn(f\"You need to install 'torch' to run generate\")\n        return\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n    streamer = TextStreamer(tokenizer) if stream else None\n    if not self.model_name_or_path or self.model_name_or_path != model_name_or_path:\n        self.model_name_or_path = model_name_or_path\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name_or_path,\n                trust_remote_code=trust_remote_code,\n                torch_dtype=torch_dtype,\n            ).to(device)\n        except:\n            warnings.warn(\n                \"Loading model to CPU instead of GPU since GPU is not available.\"\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name_or_path,\n                trust_remote_code=trust_remote_code,\n                torch_dtype=torch_dtype,\n            ).to(\"cpu\")\n\n    if not prompt:\n        prompt = [{\"role\": \"user\", \"content\": text}]\n    inputs = tokenizer.apply_chat_template(\n        prompt, add_generation_prompt=True, return_tensors=\"pt\"\n    )\n\n    tokens = self.model.generate(\n        inputs.to(self.model.device),\n        max_new_tokens=1024,\n        temperature=0.8,\n        do_sample=True,\n        streamer=streamer,\n        **kwargs,\n    )\n\n    return tokenizer.decode(tokens[0])\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_averages","title":"<code>get_averages(self, sort_by='cuda_time_total', header=None, row_limit=100, max_src_column_width=75, max_name_column_width=55, max_shapes_column_width=80, top_level_events_only=False)</code>","text":"<p>Retrieves a DataFrame of average statistics from the PyTorch profiler powered by Kineto.</p> <p>Parameters:</p> Name Type Description Default <code>sort_by</code> <code>str</code> <p>The attribute to sort the data by. Defaults to 'cuda_time_total'.</p> <code>'cuda_time_total'</code> <code>header</code> <code>str</code> <p>Header for the DataFrame. Defaults to None.</p> <code>None</code> <code>row_limit</code> <code>int</code> <p>The maximum number of rows to return. Defaults to 100.</p> <code>100</code> <code>max_src_column_width</code> <code>int</code> <p>Maximum width for the source column. Defaults to 75.</p> <code>75</code> <code>max_name_column_width</code> <code>int</code> <p>Maximum width for the name column. Defaults to 55.</p> <code>55</code> <code>max_shapes_column_width</code> <code>int</code> <p>Maximum width for the shapes column. Defaults to 80.</p> <code>80</code> <code>top_level_events_only</code> <code>bool</code> <p>Whether to include only top-level events. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>A DataFrame containing the averaged profiler statistics.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_averages(\n    self,\n    sort_by=\"cuda_time_total\",\n    header=None,\n    row_limit=100,\n    max_src_column_width=75,\n    max_name_column_width=55,\n    max_shapes_column_width=80,\n    top_level_events_only=False,\n):\n    \"\"\"\n    Retrieves a DataFrame of average statistics from the PyTorch profiler powered by Kineto.\n\n    Args:\n        sort_by (str): The attribute to sort the data by. Defaults to 'cuda_time_total'.\n        header (str, optional): Header for the DataFrame. Defaults to None.\n        row_limit (int): The maximum number of rows to return. Defaults to 100.\n        max_src_column_width (int): Maximum width for the source column. Defaults to 75.\n        max_name_column_width (int): Maximum width for the name column. Defaults to 55.\n        max_shapes_column_width (int): Maximum width for the shapes column. Defaults to 80.\n        top_level_events_only (bool): Whether to include only top-level events. Defaults to False.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the averaged profiler statistics.\n    \"\"\"\n    df_averages, self.dict_averages, str_averages = self.pt_profiler.get_averages(\n        sort_by=\"cuda_time_total\",\n        header=None,\n        row_limit=100,\n        max_src_column_width=75,\n        max_name_column_width=55,\n        max_shapes_column_width=80,\n        top_level_events_only=False,\n    )\n    return df_averages\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_jax_traces","title":"<code>get_jax_traces(self)</code>","text":"<p>Retrieves a list of JAX trace directories sorted by date.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of directory names containing JAX traces.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_jax_traces(self):\n    \"\"\"\n    Retrieves a list of JAX trace directories sorted by date.\n\n    Returns:\n        List[str]: A list of directory names containing JAX traces.\n    \"\"\"\n    # List all items in the directory\n    directory_path = self.output_path\n    if not os.path.isdir(directory_path):\n        return []\n    all_items = os.listdir(directory_path)\n\n    # Filter out items that are directories and follow the naming pattern\n    date_directories = []\n    for item in all_items:\n        if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(\n            \"jax_trace_\"\n        ):\n            # Extract the date and time part from the folder name\n            datetime_part = item.split(\"jax_trace_\")[1]\n\n            # Parse the date and time into a datetime object\n            try:\n                folder_date = datetime.datetime.strptime(\n                    datetime_part, \"%Y-%m-%d_%H-%M-%S\"\n                )\n                date_directories.append((item, folder_date))\n            except ValueError:\n                # Handle cases where the date format is incorrect or different\n                print(f\"Skipping {item} due to unexpected date format.\")\n\n    # Sort the directories by the parsed datetime\n    date_directories.sort(key=lambda x: x[1])\n\n    # Return only the directory names, in sorted order\n    return [name for name, date in date_directories]\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_profiler_stats","title":"<code>get_profiler_stats(self)</code>","text":"<p>Retrieves statistics from the PyTorch profiler.</p> <p>Returns:</p> Type Description <p>A table containing key averages of profiler statistics, particularly focusing on CUDA time.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_profiler_stats(self):\n    \"\"\"\n    Retrieves statistics from the PyTorch profiler.\n\n    Returns:\n        A table containing key averages of profiler statistics, particularly focusing on CUDA time.\n    \"\"\"\n    stat_table = self.pt_profiler.key_averages().table(\n        sort_by=\"cuda_time_total\", row_limit=10\n    )\n    return stat_table\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_pt_traces","title":"<code>get_pt_traces(self)</code>","text":"<p>Retrieves a list of PyTorch trace directories sorted by date.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of directory names containing PyTorch traces.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_pt_traces(self):\n    \"\"\"\n    Retrieves a list of PyTorch trace directories sorted by date.\n\n    Returns:\n        List[str]: A list of directory names containing PyTorch traces.\n    \"\"\"\n    # List all items in the directory\n    directory_path = self.output_path\n    if not os.path.isdir(directory_path):\n        return []\n    all_items = os.listdir(directory_path)\n\n    # Filter out items that are directories and follow the naming pattern\n    date_directories = []\n    for item in all_items:\n        if os.path.isdir(os.path.join(directory_path, item)) and item.startswith(\n            \"pt_trace_\"\n        ):\n            # Extract the date and time part from the folder name\n            datetime_part = item.split(\"pt_trace_\")[1]\n\n            # Parse the date and time into a datetime object\n            try:\n                folder_date = datetime.datetime.strptime(\n                    datetime_part, \"%Y-%m-%d_%H-%M-%S\"\n                )\n                date_directories.append((item, folder_date))\n            except ValueError:\n                # Handle cases where the date format is incorrect or different\n                print(f\"Skipping {item} due to unexpected date format.\")\n\n    # Sort the directories by the parsed datetime\n    date_directories.sort(key=lambda x: x[1])\n\n    # Return only the directory names, in sorted order\n    return [name for name, date in date_directories]\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_servers","title":"<code>get_servers(self)</code>","text":"<p>Retrieves the list of available servers from the remote service.</p> <p>Returns:</p> Type Description <p>A list of servers if successful, or raises an exception if the user is not authenticated.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_servers(self):\n    \"\"\"\n    Retrieves the list of available servers from the remote service.\n\n    Returns:\n        A list of servers if successful, or raises an exception if the user is not authenticated.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    return self.client.get_servers()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.get_service","title":"<code>get_service(self, server_name=None)</code>","text":"<p>Retrieves the service URL for a running service or app on the server.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The name of the server. If None, uses the server name set in the instance.</p> <code>None</code> <p>Returns:</p> Type Description <p>The URL to access the running service or app on the server.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def get_service(self, server_name: str = None):\n    \"\"\"\n    Retrieves the service URL for a running service or app on the server.\n\n    Args:\n        server_name (str): The name of the server. If None, uses the server name set in the instance.\n\n    Returns:\n        The URL to access the running service or app on the server.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    if server_name is None:\n        server_name = self.server_name\n    return f\"If you have running service or app in your server, check it out here -&gt; {self.client.get_service(server_name=server_name)}\"\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.load_view","title":"<code>load_view(self, dir_name=None)</code>","text":"<p>Loads a view of the profiler data onto your remote server.</p> <p>Parameters:</p> Name Type Description Default <code>dir_name</code> <code>str</code> <p>The directory name to load the view from. If None, generates a timestamp-based directory name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A URL to the GPU usage statistics dashboard.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def load_view(self, dir_name: str = None):\n    \"\"\"\n    Loads a view of the profiler data onto your remote server.\n\n    Args:\n        dir_name (str, optional): The directory name to load the view from. If None, generates a timestamp-based directory name. Defaults to None.\n\n    Returns:\n        str: A URL to the GPU usage statistics dashboard.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    formatted_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    if not dir_name:\n        view_path = f\"{formatted_datetime}.alphai\"\n    else:\n        view_path = dir_name\n    self.client.post_contents(path=\"\", ext=\".alphai\", type=\"directory\")\n    self.client.patch_contents(path=\"Untitled Folder.alphai\", new_path=view_path)\n    self.client.put_contents(\n        path=view_path,\n        file_path=f\"{self.pt_profiler.profiler_path}/profiling.alphai\",\n    )\n    return f\"Check out your GPU usage statistics at -&gt; https://dashboard.amdatascience.com/agent-alph\"\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.memory_requirement","title":"<code>memory_requirement(self, model_name_or_path='stabilityai/stablelm-zephyr-3b', device='cuda', trust_remote_code=True, torch_dtype='auto')</code>","text":"<p>Estimates and prints the memory requirement for a specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The name or path of the model to be loaded. Defaults to 'stabilityai/stablelm-zephyr-3b'.</p> <code>'stabilityai/stablelm-zephyr-3b'</code> <code>device</code> <code>str</code> <p>The device to load the model on ('cuda' or 'cpu'). Defaults to 'cuda'.</p> <code>'cuda'</code> <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading the model. Defaults to True.</p> <code>True</code> <code>torch_dtype</code> <code>str</code> <p>The data type for the model parameters. Defaults to 'auto'.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the memory requirement in MB and GB.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def memory_requirement(\n    self,\n    model_name_or_path: str = \"stabilityai/stablelm-zephyr-3b\",\n    device: str = \"cuda\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n):\n    \"\"\"\n    Estimates and prints the memory requirement for a specified model.\n\n    Args:\n        model_name_or_path (str): The name or path of the model to be loaded. Defaults to 'stabilityai/stablelm-zephyr-3b'.\n        device (str): The device to load the model on ('cuda' or 'cpu'). Defaults to 'cuda'.\n        trust_remote_code (bool): Whether to trust remote code when loading the model. Defaults to True.\n        torch_dtype (str): The data type for the model parameters. Defaults to 'auto'.\n\n    Returns:\n        dict: A dictionary containing the memory requirement in MB and GB.\n    \"\"\"\n    if not is_package_installed(\"torch\"):\n        warnings.warn(f\"You need to install 'torch' to run memory_requirement\")\n        return\n    if not self.model_name_or_path or self.model_name_or_path != model_name_or_path:\n        self.model_name_or_path = model_name_or_path\n        try:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name_or_path,\n                trust_remote_code=trust_remote_code,\n                torch_dtype=torch_dtype,\n            ).to(device)\n        except:\n            warnings.warn(\n                \"Loading model to CPU instead of GPU since GPU is not available.\"\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name_or_path,\n                trust_remote_code=trust_remote_code,\n                torch_dtype=torch_dtype,\n            ).to(\"cpu\")\n    try:\n        param_value = self.model.num_parameters()\n    except:\n        param_value = sum(p.numel() for p in self.model.parameters())\n\n    megabyte_value = param_value * 2 / 1000000\n    gigabyte_value = param_value * 2 / 1000000000\n    print(\n        f\"Memory requirement assuming float16 dtype for {model_name_or_path}: {megabyte_value:.2f} MB or {gigabyte_value:.2f} GB\"\n    )\n    return {\n        \"model_name_or_path\": model_name_or_path,\n        \"memory_requirement_mb_float16\": f\"{megabyte_value:.2f} MB\",\n        \"memory_requirement_gb_float16\": f\"{gigabyte_value:.2f} GB\",\n    }\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.run_code","title":"<code>run_code(self, code=\"print('Hello world!')\", server_name=None, clear_other_kernels=True, return_full=False)</code>","text":"<p>Executes the given code on a remote server.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to execute. If a file path is provided, the code in the file is executed.</p> <code>\"print('Hello world!')\"</code> <code>server_name</code> <code>str</code> <p>The name of the server where the code will be executed. If None, uses the server name set in the instance.</p> <code>None</code> <code>clear_other_kernels</code> <code>bool</code> <p>Whether to shut down other kernels on the server before executing the code.</p> <code>True</code> <code>return_full</code> <code>bool</code> <p>Whether to return the full response from the server.</p> <code>False</code> <p>Returns:</p> Type Description <p>The output from the code execution.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def run_code(\n    self,\n    code: str = \"print('Hello world!')\",\n    server_name: str = None,\n    clear_other_kernels: bool = True,\n    return_full: bool = False,\n):\n    \"\"\"\n    Executes the given code on a remote server.\n\n    Args:\n        code (str): The code to execute. If a file path is provided, the code in the file is executed.\n        server_name (str): The name of the server where the code will be executed. If None, uses the server name set in the instance.\n        clear_other_kernels (bool): Whether to shut down other kernels on the server before executing the code.\n        return_full (bool): Whether to return the full response from the server.\n\n    Returns:\n        The output from the code execution.\n    \"\"\"\n    # Use set self.server_name if not provided\n    if server_name is None:\n        server_name = self.server_name\n    if clear_other_kernels:\n        self.client.shutdown_all_kernels(server_name=server_name)\n    if os.path.isfile(code):\n        if os.path.splitext(code)[1] != \".py\":\n            warnings.warn(\n                \"This doesn't seem to be a python file, but will try to run it anyway.\"\n            )\n        with open(code, \"r\") as f:\n            code = f.read()\n    return self.client.send_channel_execute(\n        server_name=server_name, messages=[code], return_full=return_full\n    )\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.run_profiler_analysis","title":"<code>run_profiler_analysis(self, trace_path=None, visualize=False)</code>","text":"<p>Runs an analysis of the profiler data and optionally visualizes the results.</p> <p>Parameters:</p> Name Type Description Default <code>trace_path</code> <code>str</code> <p>The path to the trace data. If None, uses the latest trace. Defaults to None.</p> <code>None</code> <code>visualize</code> <code>bool</code> <p>Whether to visualize the analysis results. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of DataFrames containing various analysis results, such as idle time, temporal breakdown, and GPU kernel breakdown.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def run_profiler_analysis(self, trace_path: str = None, visualize: bool = False):\n    \"\"\"\n    Runs an analysis of the profiler data and optionally visualizes the results.\n\n    Args:\n        trace_path (str, optional): The path to the trace data. If None, uses the latest trace. Defaults to None.\n        visualize (bool): Whether to visualize the analysis results. Defaults to False.\n\n    Returns:\n        A tuple of DataFrames containing various analysis results, such as idle time, temporal breakdown, and GPU kernel breakdown.\n    \"\"\"\n    if trace_path:\n        pt_trace_dirs = [trace_path]\n    else:\n        pt_trace_dirs = self.get_pt_traces()\n    if pt_trace_dirs:\n        try:\n            trace_dir = os.path.join(self.pt_profiler.dir_path, pt_trace_dirs[-1])\n            self.analyzer = TraceAnalysis(trace_dir=trace_dir)\n            idle_time_df = self.analyzer.get_idle_time_breakdown(\n                show_idle_interval_stats=True, visualize=visualize\n            )\n            time_spent_df = self.analyzer.get_temporal_breakdown(\n                visualize=visualize\n            )\n            (\n                kernel_type_metrics_df,\n                kernel_metrics_df,\n            ) = self.analyzer.get_gpu_kernel_breakdown()\n            self.dict_idle_time = idle_time_df[0].to_dict()\n            self.dict_time_spent = time_spent_df.to_dict()\n            self.dict_type_metrics = kernel_type_metrics_df.to_dict()\n            self.dict_kernel_metrics = kernel_metrics_df.to_dict()\n            return (\n                idle_time_df,\n                time_spent_df,\n                kernel_type_metrics_df,\n                kernel_metrics_df,\n            )\n        except:\n            warnings.warn(\n                \"Error running profiler analysis, may not have GPU trace data so will continue without it.\"\n            )\n            self.dict_idle_time = {}\n            self.dict_time_spent = {}\n            self.dict_type_metrics = {}\n            self.dict_kernel_metrics = {}\n            return\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.save","title":"<code>save(self, return_results=False)</code>","text":"<p>Saves the profiler data and analysis results to a specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>return_results</code> <code>bool</code> <p>Whether to return the saved data as a dictionary. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict (optional)</code> <p>A dictionary containing the saved data if return_results is True.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def save(self, return_results: bool = False):\n    \"\"\"\n    Saves the profiler data and analysis results to a specified directory.\n\n    Args:\n        return_results (bool): Whether to return the saved data as a dictionary. Defaults to False.\n\n    Returns:\n        dict (optional): A dictionary containing the saved data if return_results is True.\n    \"\"\"\n    alphai_dict = {}\n    if self.dict_idle_time is None:\n        warnings.warn(\n            \"Make sure to run_profiler_analysis() before saving to your analytics.\"\n        )\n        self.run_profiler_analysis()\n    self.get_averages()\n    alphai_dict[\"metadata\"] = self.analyzer.t.meta_data\n    alphai_dict[\"idle_time\"] = self.dict_idle_time\n    alphai_dict[\"time_spent\"] = self.dict_time_spent\n    alphai_dict[\"type_metrics\"] = self.dict_type_metrics\n    alphai_dict[\"kernel_metrics\"] = self.dict_kernel_metrics\n    alphai_dict[\"key_averages\"] = self.dict_averages\n    with open(\n        os.path.join(self.pt_profiler.profiler_path, \"profiling.alphai\"), \"w\"\n    ) as f:\n        json.dump(alphai_dict, f, indent=4)\n    if return_results:\n        return alphai_dict\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.start","title":"<code>start(self, tensor_backend=None)</code>","text":"<p>Starts the profiler for the specified tensor backend.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_backend</code> <code>str</code> <p>The backend to use for profiling ('torch', 'jax', 'tensorflow').                    If None, defaults to an available backend.</p> <code>None</code> Source code in <code>alphai/alphai.py</code> <pre><code>def start(self, tensor_backend: str = None):\n    \"\"\"\n    Starts the profiler for the specified tensor backend.\n\n    Args:\n        tensor_backend (str): The backend to use for profiling ('torch', 'jax', 'tensorflow').\n                               If None, defaults to an available backend.\n    \"\"\"\n    # Handle if none, not installed, or unknown tensor_backend given\n    # Default to torch tensorbackend or whatever's available\n    if not tensor_backend:\n        if is_package_installed(\"torch\"):\n            tensor_backend = \"torch\"\n        elif is_package_installed(\"jax\"):\n            tensor_backend = \"jax\"\n        elif is_package_installed(\"tensorflow\"):\n            tensor_backend = \"tensorflow\"\n        else:\n            warnings.warn(\n                f\"Tensor framework must first be installed from a supported library: {self.supported_backends} to enable profiling.\"\n            )\n            return\n    if tensor_backend not in self.supported_backends:\n        warnings.warn(\n            f\"Tensor framework is not supported, must be one of {self.supported_backends} to enable profiling.\"\n        )\n        return\n    if not is_package_installed(tensor_backend):\n        warnings.warn(f\"You need to install '{tensor_backend}' to start profiling\")\n\n    if tensor_backend == \"torch\":\n        try:\n            self.pt_profiler.start()\n        except:\n            # Try to stop hanging profiler and try again\n            self.pt_profiler.stop()\n            self.pt_profiler.start()\n    elif tensor_backend == \"jax\":\n        try:\n            self.jax_profiler.start()\n        except:\n            # Try to stop hanging profiler and try again\n            self.jax_profiler.stop()\n            self.jax_profiler.start()\n    elif tensor_backend == \"tensorflow\":\n        pass\n\n    self.tensor_backend = tensor_backend\n    self.profiler_started = True\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.start_server","title":"<code>start_server(self, server_name=None, environment='ai', server_request='medium-cpu')</code>","text":"<p>Starts a server with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The name of the server to start. If None, uses the server name set in the instance.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the server start request.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def start_server(\n        self,\n        server_name: str = None,\n        environment: str = \"ai\",\n        server_request: str = \"medium-cpu\",\n    ):\n    \"\"\"\n    Starts a server with the given name.\n\n    Args:\n        server_name (str): The name of the server to start. If None, uses the server name set in the instance.\n\n    Returns:\n        Response from the server start request.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    # Use set self.server_name if not provided\n    if server_name is None:\n        server_name = self.server_name\n    return self.client.start_server(server_name=server_name, environment=environment, server_request=server_request)\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.start_timer","title":"<code>start_timer(self)</code>","text":"<p>Starts the benchmarking timer.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def start_timer(self):\n    \"\"\"\n    Starts the benchmarking timer.\n    \"\"\"\n    self.benchmarker.start()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.step","title":"<code>step(self)</code>","text":"<p>Advances the profiler by one step. Mainly used for the PyTorch profiler.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def step(self):\n    \"\"\"\n    Advances the profiler by one step. Mainly used for the PyTorch profiler.\n    \"\"\"\n    self.pt_profiler.step()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.stop","title":"<code>stop(self)</code>","text":"<p>Stops the currently running profiler.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stops the currently running profiler.\n    \"\"\"\n    if not self.profiler_started or not self.tensor_backend:\n        warnings.warn(f\"Profiler never started\")\n        return\n\n    if self.tensor_backend == \"torch\":\n        self.pt_profiler.stop()\n    elif self.tensor_backend == \"jax\":\n        self.jax_profiler.stop()\n    elif self.tensor_backend == \"tensorflow\":\n        pass\n\n    self.profiler_started = False\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.stop_server","title":"<code>stop_server(self, server_name=None)</code>","text":"<p>Stops a server with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The name of the server to stop. If None, uses the server name set in the instance.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the server stop request.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def stop_server(self, server_name: str = None):\n    \"\"\"\n    Stops a server with the given name.\n\n    Args:\n        server_name (str): The name of the server to stop. If None, uses the server name set in the instance.\n\n    Returns:\n        Response from the server stop request.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    # Use set self.server_name if not provided\n    if server_name is None:\n        server_name = self.server_name\n    return self.client.stop_server(server_name=server_name)\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.stop_timer","title":"<code>stop_timer(self, print_results=True)</code>","text":"<p>Stops the timer and optionally prints the results.</p> <p>Parameters:</p> Name Type Description Default <code>print_results</code> <code>bool</code> <p>Whether to print the results. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>The results of the benchmark.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def stop_timer(self, print_results: bool = True):\n    \"\"\"\n    Stops the timer and optionally prints the results.\n\n    Args:\n        print_results (bool): Whether to print the results. Defaults to True.\n\n    Returns:\n        The results of the benchmark.\n    \"\"\"\n    return self.benchmarker.stop()\n</code></pre>"},{"location":"api-reference/alphai/#alphai.alphai.AlphAI.upload","title":"<code>upload(self, server_name=None, file_path='', remote_path='')</code>","text":"<p>Uploads a file to a remote server.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The name of the server to which the file will be uploaded. If None, uses the server name set in the instance.</p> <code>None</code> <code>file_path</code> <code>str</code> <p>The local path to the file.</p> <code>''</code> <code>remote_path</code> <code>str</code> <p>The remote path where the file will be stored.</p> <code>''</code> <p>Returns:</p> Type Description <p>The response from the upload request.</p> Source code in <code>alphai/alphai.py</code> <pre><code>def upload(self, server_name: str = None, file_path: str = \"\", remote_path=\"\"):\n    \"\"\"\n    Uploads a file to a remote server.\n\n    Args:\n        server_name (str): The name of the server to which the file will be uploaded. If None, uses the server name set in the instance.\n        file_path (str): The local path to the file.\n        remote_path (str): The remote path where the file will be stored.\n\n    Returns:\n        The response from the upload request.\n    \"\"\"\n    if not self.api_key:\n        raise ValueError(\"Requires user authentication with an API Key\")\n    # Use set self.server_name if not provided\n    if server_name is None:\n        server_name = self.server_name\n    return self.client.put_contents(\n        server_name=server_name, path=remote_path, file_path=file_path\n    )\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"models/hugging-face/","title":"Hugging Face","text":"<p>AlphAI includes many useful integrations with Hugging Face's open source libraries and AI community. Check out what you can do below</p> <pre><code>import os\nfrom alphai import AlphAI\n\naai = AlphAI()\n</code></pre>"},{"location":"models/hugging-face/#estimate-memory-requirement","title":"Estimate Memory Requirement","text":"<p>Fun fact: the number of parameters you see in a the name of the model i.e. 3B, 7B, 40B, can easily be converted into a rough estimate of the minimum GB required for your GPU VRAM. Just mulitply by the number by 2 if the parameter dtype is float16 or by 4 if float32!</p> <p>We've got a fun little helper function to estimate it without even loading the model.</p> <pre><code>aai.estimate_memory_requirement(model_name=\"stabilityai/stablelm-zephyr-3b\")\n</code></pre>"},{"location":"models/hugging-face/#run-generate","title":"Run Generate","text":"<p>You can run local LLMs on GPUs, and we integrate with many open source Hugging Face models.</p> <pre><code>output = aai.generate(\"I need a python function that generates the fibonacci sequence recursively.\")\n</code></pre>"},{"location":"models/hugging-face/#memory-requirement","title":"Memory Requirement","text":"<p>Now you can get a fairly more accurate size of the model and memory requirement now that the model is loaded.</p> <pre><code>aai.memory_requirement(model_name_or_path=\"stabilityai/stablelm-zephyr-3b\")\n</code></pre> <pre><code>\n</code></pre>"}]}